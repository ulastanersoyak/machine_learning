{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZL5HI3Rj3blg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Caner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import local_utils\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Instantiate MiniResNet.\n",
    "\n",
    "Apply evaluation mode (method `.eval()`) to prevent of batch normalization layers parameters changes.\n",
    "\n",
    "Load state dict with mapping location to cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H1qAmZ7D3jMx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your model\n",
    "# apply eval() method\n",
    "net = local_utils.MiniResNet()\n",
    "net.eval()\n",
    "net.load_state_dict(torch.load(\"weights.pth\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Instantiate train and test loaders with batch size = 1.\n",
    "\n",
    "Extract 5% of training data (iterate in for loop to get a random samples).\n",
    "\n",
    "Collect data and labels as lists.\n",
    "\n",
    "Concatenate both lists (separately).\n",
    "\n",
    "Initialize LoaderWrapper with results of concatenation and batch size = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "L4tpsFQX4jip"
   },
   "outputs": [],
   "source": [
    "# concatenation: torch.cat()\n",
    "\n",
    "\n",
    "class LoaderWrapper:\n",
    "    def __init__(self, data, labels, batch_size=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if index >= len(self):\n",
    "            raise StopIteration()\n",
    "\n",
    "        beg = index*self.batch_size\n",
    "        end = beg+self.batch_size\n",
    "        return self.data[beg:end], self.labels[beg:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.batch_size\n",
    "\n",
    "\n",
    "train_loader = local_utils.get_train_dataset(batch_size=1)          \n",
    "test_loader = local_utils.get_test_dataset(batch_size=1)   \n",
    "\n",
    "quantization_data = []      \n",
    "quantization_labels = []   \n",
    "\n",
    "for X,y in train_loader:\n",
    "    quantization_data.append(X)\n",
    "    quantization_labels.append(y) \n",
    "\n",
    "    if len(quantization_data)== ((len(train_loader)*5)/100):\n",
    "        break\n",
    "\n",
    "quantization_loader = LoaderWrapper(quantization_data,quantization_labels)\n",
    "\n",
    "del train_loader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Instantiate accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = local_utils.AccuracyMetic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QFuGuk2NB4fz",
    "outputId": "3aea7dc1-a5e2-4f08-e7e2-f2af247666e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model,\n",
    "             dataloader,\n",
    "             evaluator\n",
    "             ):\n",
    "    \"\"\"\n",
    "    :param model: torch.nn.Model\n",
    "    :param dataloader: data generator / loader\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Evaluation\", len(dataloader))\n",
    "    with torch.no_grad(), tm:\n",
    "        score = 0.0\n",
    "        cntr = 0\n",
    "        for i, XY in enumerate(dataloader):\n",
    "            X = XY[0]\n",
    "            Y = XY[1:]\n",
    "            y_pred = model(X)\n",
    "            score = score*cntr + X.shape[0]*evaluator(y_pred, *Y)\n",
    "            cntr += X.shape[0]\n",
    "            score /= cntr\n",
    "            print(\"\\rEvaluation {}/{}. Score = {}\".format(i,len(dataloader), score),end='')\n",
    "        \n",
    "        print(\"\\rEvaluation {}/{}. Score = {}\".format(len(dataloader),len(dataloader), score),end='\\n')\n",
    "    print(tm)\n",
    "\n",
    "\n",
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device,\n",
    "             dataloader,\n",
    "             evaluator):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader - for 'calib' must be batch_size == 1\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Quantization\", len(dataloader))\n",
    "    with tm:\n",
    "        # available in docker or after packaging \n",
    "        # vitis-AI-tools/..../pytorch../pytorch_nndct\n",
    "        # and installing the package\n",
    "        from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "        # model to device\n",
    "        model = float_model.to(device)\n",
    "\n",
    "        # That was present in vai tutorial.\n",
    "        # I don't know if it affects to anything?\n",
    "        # Force to merge BN with CONV for better quantization accuracy\n",
    "        optimize = 1\n",
    "\n",
    "        rand_in = torch.randn(input_shape)\n",
    "        print(\"get qunatizer start\")\n",
    "        try:\n",
    "            quantizer = torch_quantizer(\n",
    "                quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "        except Exception as e:\n",
    "            print(\"exception:\")\n",
    "            print(e)\n",
    "            return\n",
    "        print(\"get qunatizer end\")\n",
    "\n",
    "        print(\"get quantized model start\")\n",
    "        quantized_model = quantizer.quant_model\n",
    "        print(\"get quantized model end\")\n",
    "\n",
    "        # evaluate\n",
    "        print(\"testing st\")\n",
    "        evaluate(quantized_model, dataloader, evaluator)\n",
    "        print(\"testing end\")\n",
    "\n",
    "        # export config\n",
    "        if quant_mode == 'calib':\n",
    "            print(\"export config\")\n",
    "            quantizer.export_quant_config()\n",
    "            print(\"export config end\")\n",
    "        # export model\n",
    "        if quant_mode == 'test':\n",
    "            print(\"export xmodel\")\n",
    "            quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "            print(\"export xmodel end\")\n",
    "    print(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pytorch_nndct (from versions: none)\n",
      "ERROR: No matching distribution found for pytorch_nndct\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch_nndct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Evaluate network floating-point model with  test loader and quantization loader with accuracy evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PeaKZrJc3blk",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation 10000/10000. Score = 0.8385004997253418\n",
      "Execution time: 0:0:23:0, processed 10000 frames, throughput: 434.7826086956522 fps.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# You can evaluate your floating point model first \u001b[39;00m\n\u001b[0;32m      2\u001b[0m evaluate(model\u001b[39m=\u001b[39mnet , dataloader\u001b[39m=\u001b[39m test_loader, evaluator\u001b[39m=\u001b[39mmetric)\n\u001b[1;32m----> 3\u001b[0m evaluate(model\u001b[39m=\u001b[39;49mnet , dataloader\u001b[39m=\u001b[39;49m quantization_loader, evaluator\u001b[39m=\u001b[39;49mmetric)\n",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, dataloader, evaluator)\u001b[0m\n\u001b[0;32m     15\u001b[0m X \u001b[39m=\u001b[39m XY[\u001b[39m0\u001b[39m]\n\u001b[0;32m     16\u001b[0m Y \u001b[39m=\u001b[39m XY[\u001b[39m1\u001b[39m:]\n\u001b[1;32m---> 17\u001b[0m y_pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m     18\u001b[0m score \u001b[39m=\u001b[39m score\u001b[39m*\u001b[39mcntr \u001b[39m+\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39mevaluator(y_pred, \u001b[39m*\u001b[39mY)\n\u001b[0;32m     19\u001b[0m cntr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Caner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Caner\\Desktop\\embedded ai labları\\lab_11\\local_utils.py:348\u001b[0m, in \u001b[0;36mMiniResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 348\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFC(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mCNN(x))\n",
      "File \u001b[1;32mc:\\Users\\Caner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Caner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Caner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Caner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Caner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "# You can evaluate your floating point model first \n",
    "evaluate(model=net , dataloader= test_loader, evaluator=metric)\n",
    "evaluate(model=net , dataloader= quantization_loader, evaluator=metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vitis AI Quantizer for Post Training Quantization uses two stages.\n",
    "\n",
    "First is `calib` mode (calibration) - VAI Quantizer parses the model and adjust quantization parameters.\n",
    "\n",
    "Second is evaluation / test mode - after this step \n",
    "\n",
    "(theoretically after check is there is not too much of accuracy loss) \n",
    "\n",
    "model is exported in onnx format.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Run quantization for network in 'calib' mode with input shape = [1, 1, 28, 28].\n",
    "\n",
    "Use quantization loader and accuracy metric evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UqCYtQlO3bll",
    "outputId": "7103e738-593a-4300-81ed-3acf6a7387e3",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_nndct'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Quantize model - calib - is slow\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m quantize(float_model\u001b[39m=\u001b[39;49mnet,\n\u001b[0;32m      3\u001b[0m          input_shape\u001b[39m=\u001b[39;49m [\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m28\u001b[39;49m, \u001b[39m28\u001b[39;49m],\n\u001b[0;32m      4\u001b[0m          quant_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcalib\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m          dataloader\u001b[39m=\u001b[39;49mquantization_loader,\n\u001b[0;32m      6\u001b[0m          evaluator\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m      7\u001b[0m          device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m      8\u001b[0m          quant_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mquant_dir\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m# directory for quantizer results\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m          )\n",
      "Cell \u001b[1;32mIn[16], line 47\u001b[0m, in \u001b[0;36mquantize\u001b[1;34m(float_model, input_shape, quant_dir, quant_mode, device, dataloader, evaluator)\u001b[0m\n\u001b[0;32m     42\u001b[0m tm \u001b[39m=\u001b[39m local_utils\u001b[39m.\u001b[39mTimeMeasurement(\u001b[39m\"\u001b[39m\u001b[39mQuantization\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(dataloader))\n\u001b[0;32m     43\u001b[0m \u001b[39mwith\u001b[39;00m tm:\n\u001b[0;32m     44\u001b[0m     \u001b[39m# available in docker or after packaging \u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[39m# vitis-AI-tools/..../pytorch../pytorch_nndct\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[39m# and installing the package\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_nndct\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapis\u001b[39;00m \u001b[39mimport\u001b[39;00m torch_quantizer, dump_xmodel\n\u001b[0;32m     48\u001b[0m     \u001b[39m# model to device\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     model \u001b[39m=\u001b[39m float_model\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_nndct'"
     ]
    }
   ],
   "source": [
    "# Quantize model - calib - is slow\n",
    "quantize(float_model=net,\n",
    "         input_shape= [1, 1, 28, 28],\n",
    "         quant_mode=\"calib\",\n",
    "         dataloader=quantization_loader,\n",
    "         evaluator=metric,\n",
    "         device=device,\n",
    "         quant_dir='quant_dir', # directory for quantizer results\n",
    "         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Run quantization in `test` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hW7mQGoL3blm",
    "outputId": "ffe3ef85-8078-4f53-87cc-62a3b036c36e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing MiniResNet...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quant_dir/MiniResNet.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 3001/3001. Score = 0.9880039691925049\n",
      "Execution time: 0:0:17:0, processed 3001 frames, throughput: 176.52941176470588 fps.\n",
      "testing end\n",
      "export xmodel\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Successfully convert 'MiniResNet' to xmodel.(quant_dir/MiniResNet_int.xmodel)\u001b[0m\n",
      "export xmodel end\n",
      "Execution time: 0:0:17:0, processed 3001 frames, throughput: 176.52941176470588 fps.\n"
     ]
    }
   ],
   "source": [
    "# Quantize model - test - is faster\n",
    "\n",
    "quantize(float_model=net,\n",
    "         input_shape= [1, 1, 28, 28],\n",
    "         quant_mode=\"test\",\n",
    "         dataloader=quantization_loader,\n",
    "         evaluator=metric,\n",
    "         device=device,\n",
    "         quant_dir='quant_dir', # directory for quantizer results\n",
    "         )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Compile the quantized model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MApToNQ3bln",
    "outputId": "aa19a1c1-66a9-465f-cb22-43214b612ce5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'vai_c_xir' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "# --xmodel quant_dir+'/'+{python class model name}+'_int.xmodel' - the result of quantization\n",
    "# --arch file dpu fingerprint (denotes DPU architecture and supported operations) - *.json file \n",
    "# --net_name name of network  - any name\n",
    "# --output_dir directory where results will be stored\n",
    "!vai_c_xir --xmodel ... --arch arch.json --net_name ... --output_dir  build"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ISD7IjSg3blp"
   },
   "source": [
    "13. Save this file. Close Jupyter server. Exit from Vitis AI docker environment (`exit` command)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Quantize.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "814e3fea5aed7c4aab3ab1a9ee8ce6861cb7a1485cdc7c835a0bbd4579d59261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
